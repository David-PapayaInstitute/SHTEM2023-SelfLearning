{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68089,"status":"ok","timestamp":1692752814637,"user":{"displayName":"David J Florez R","userId":"13874950378236808965"},"user_tz":240},"id":"PtNQnvYHO5-7","outputId":"77f8487a-cd72-4104-aadd-1cf2f1e46ac7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","['breastcancerX.npy', 'breastcancerY.npy', 'alzheimerY.npy', 'alzheimerX.npy', 'covidX.npy', 'covidY.npy', 'cifar10X.npy', 'cifar10Y.npy', 'glaucomaX.npy', 'glaucomaY.npy', 'applescabX.npy', 'applescabY.npy']\n"]}],"source":["from tensorflow.keras import layers\n","from tensorflow.keras import regularizers\n","import tensorflow as tf\n","import os\n","import time\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","\n","!wget -q https://git.io/JYx2x -O resnet_cifar10_v2.py\n","import resnet_cifar10_v2\n","\n","drive.mount('/content/drive/')\n","loc = 'drive/MyDrive/Shtem2023/'\n","print(os.listdir(loc+'/all_data'))"]},{"cell_type":"markdown","metadata":{"id":"iH9tpShoSraK"},"source":["Sourced from https://keras.io/examples/vision/simsiam/"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2927,"status":"ok","timestamp":1692752817558,"user":{"displayName":"David J Florez R","userId":"13874950378236808965"},"user_tz":240},"id":"Bnh5dVFdNmpr"},"outputs":[],"source":["\n","\n","##########    THIS IS THE ONLY CELL YOU NEED TO CHANGE :) ################\n","\n","dataset_name = 'glaucoma'\n","X = np.load(loc+'/all_data/'+dataset_name+'X.npy')\n","Y = np.load(loc+'/all_data/'+dataset_name+'Y.npy')\n","num_cat = 2\n","CROP_TO = 250"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1692752817558,"user":{"displayName":"David J Florez R","userId":"13874950378236808965"},"user_tz":240},"id":"VBiM_YzFPIHn"},"outputs":[],"source":["AUTO = tf.data.AUTOTUNE\n","BATCH_SIZE = 4 # maybe up to 256 # lol no, don't change from 8\n","Ntest = 208 # make sure Ntrain + Ntest < dataset size!!!!! # WANT NTRAIN ~ 200-1000\n","EPOCHS = 15\n","SEED = 26\n","PROJECT_DIM = 128\n","LATENT_DIM = 64\n","WEIGHT_DECAY = 0.0005\n","#(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j6b0Zkk-RpJY","executionInfo":{"status":"ok","timestamp":1692763718897,"user_tz":240,"elapsed":10901344,"user":{"displayName":"David J Florez R","userId":"13874950378236808965"}},"outputId":"352b4110-27e4-46c1-fb78-d83cb76b02b8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"linear_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_4 (InputLayer)        [(None, 250, 250, 3)]     0         \n","                                                                 \n"," model_1 (Functional)        (None, 256)               568368    \n","                                                                 \n"," dense_5 (Dense)             (None, 2)                 514       \n","                                                                 \n","=================================================================\n","Total params: 568,882\n","Trainable params: 565,874\n","Non-trainable params: 3,008\n","_________________________________________________________________\n","Epoch 1/15\n","96/96 [==============================] - 167s 2s/step - loss: 3.5298 - accuracy: 0.5833 - val_loss: 4.0418 - val_accuracy: 0.5337\n","Epoch 2/15\n","96/96 [==============================] - 178s 2s/step - loss: nan - accuracy: 0.5208 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 3/15\n","96/96 [==============================] - 169s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 4/15\n","96/96 [==============================] - 168s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 5/15\n","96/96 [==============================] - 179s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 6/15\n","96/96 [==============================] - 168s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 7/15\n","96/96 [==============================] - 167s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 8/15\n","96/96 [==============================] - 168s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 9/15\n","96/96 [==============================] - 168s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 10/15\n","96/96 [==============================] - 168s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 11/15\n","96/96 [==============================] - 167s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 12/15\n","96/96 [==============================] - 167s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 13/15\n","96/96 [==============================] - 167s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 14/15\n","96/96 [==============================] - 168s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","Epoch 15/15\n","96/96 [==============================] - 168s 2s/step - loss: nan - accuracy: 0.3438 - val_loss: nan - val_accuracy: 0.3606\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","\n","/n 2545.86 /littleNtrain: 192 /n\n","Model: \"linear_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_7 (InputLayer)        [(None, 250, 250, 3)]     0         \n","                                                                 \n"," model_2 (Functional)        (None, 256)               568368    \n","                                                                 \n"," dense_10 (Dense)            (None, 2)                 514       \n","                                                                 \n","=================================================================\n","Total params: 568,882\n","Trainable params: 565,874\n","Non-trainable params: 3,008\n","_________________________________________________________________\n","Epoch 1/15\n","100/100 [==============================] - 178s 2s/step - loss: 2.5659 - accuracy: 0.5650 - val_loss: 2.2596 - val_accuracy: 0.4183\n","Epoch 2/15\n","100/100 [==============================] - 173s 2s/step - loss: 2.0873 - accuracy: 0.6250 - val_loss: 2.0281 - val_accuracy: 0.6394\n","Epoch 3/15\n","100/100 [==============================] - 174s 2s/step - loss: 2.0865 - accuracy: 0.6150 - val_loss: 1.9825 - val_accuracy: 0.5817\n","Epoch 4/15\n","100/100 [==============================] - 185s 2s/step - loss: 1.9705 - accuracy: 0.6550 - val_loss: 1.9250 - val_accuracy: 0.6394\n","Epoch 5/15\n","100/100 [==============================] - 186s 2s/step - loss: 1.9042 - accuracy: 0.6500 - val_loss: 1.9650 - val_accuracy: 0.5769\n","Epoch 6/15\n","100/100 [==============================] - 174s 2s/step - loss: 1.8516 - accuracy: 0.6550 - val_loss: 2.4339 - val_accuracy: 0.5625\n","Epoch 7/15\n","100/100 [==============================] - 187s 2s/step - loss: 1.8104 - accuracy: 0.6700 - val_loss: 2.6258 - val_accuracy: 0.5529\n","Epoch 8/15\n","100/100 [==============================] - 173s 2s/step - loss: 1.8211 - accuracy: 0.6750 - val_loss: 2.6471 - val_accuracy: 0.5481\n","Epoch 9/15\n","100/100 [==============================] - 185s 2s/step - loss: 1.8259 - accuracy: 0.7100 - val_loss: 2.6813 - val_accuracy: 0.5481\n","Epoch 10/15\n","100/100 [==============================] - 174s 2s/step - loss: 1.8374 - accuracy: 0.6750 - val_loss: 2.6605 - val_accuracy: 0.5481\n","Epoch 11/15\n","100/100 [==============================] - 174s 2s/step - loss: 1.8217 - accuracy: 0.7100 - val_loss: 2.6381 - val_accuracy: 0.5433\n","Epoch 12/15\n","100/100 [==============================] - 185s 2s/step - loss: 1.8294 - accuracy: 0.6850 - val_loss: 2.6437 - val_accuracy: 0.5433\n","Epoch 13/15\n","100/100 [==============================] - 176s 2s/step - loss: 1.8151 - accuracy: 0.6800 - val_loss: 2.7156 - val_accuracy: 0.5433\n","Epoch 14/15\n","100/100 [==============================] - 174s 2s/step - loss: 1.8187 - accuracy: 0.6800 - val_loss: 2.7342 - val_accuracy: 0.5385\n","Epoch 15/15\n","100/100 [==============================] - 175s 2s/step - loss: 1.8303 - accuracy: 0.6900 - val_loss: 2.6877 - val_accuracy: 0.5481\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","\n","/n 5270.94 /littleNtrain: 200 /n\n","Model: \"linear_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_10 (InputLayer)       [(None, 250, 250, 3)]     0         \n","                                                                 \n"," model_3 (Functional)        (None, 256)               568368    \n","                                                                 \n"," dense_15 (Dense)            (None, 2)                 514       \n","                                                                 \n","=================================================================\n","Total params: 568,882\n","Trainable params: 565,874\n","Non-trainable params: 3,008\n","_________________________________________________________________\n","Epoch 1/15\n","104/104 [==============================] - 194s 2s/step - loss: 2.6105 - accuracy: 0.6250 - val_loss: 2.1396 - val_accuracy: 0.6394\n","Epoch 2/15\n","104/104 [==============================] - 180s 2s/step - loss: 2.0890 - accuracy: 0.6298 - val_loss: 2.0158 - val_accuracy: 0.6394\n","Epoch 3/15\n","104/104 [==============================] - 181s 2s/step - loss: 2.0015 - accuracy: 0.6298 - val_loss: 1.9128 - val_accuracy: 0.7404\n","Epoch 4/15\n","104/104 [==============================] - 190s 2s/step - loss: 1.9204 - accuracy: 0.6827 - val_loss: 1.9028 - val_accuracy: 0.6635\n","Epoch 5/15\n","104/104 [==============================] - 181s 2s/step - loss: 1.8135 - accuracy: 0.7500 - val_loss: 1.9503 - val_accuracy: 0.6779\n","Epoch 6/15\n","104/104 [==============================] - 192s 2s/step - loss: 1.7882 - accuracy: 0.7500 - val_loss: 2.0334 - val_accuracy: 0.5817\n","Epoch 7/15\n","104/104 [==============================] - 181s 2s/step - loss: 1.7535 - accuracy: 0.7500 - val_loss: 2.3132 - val_accuracy: 0.5865\n","Epoch 8/15\n","104/104 [==============================] - 192s 2s/step - loss: 1.7162 - accuracy: 0.7933 - val_loss: 2.3487 - val_accuracy: 0.6442\n","Epoch 9/15\n","104/104 [==============================] - 181s 2s/step - loss: 1.7378 - accuracy: 0.7644 - val_loss: 2.3585 - val_accuracy: 0.6635\n","Epoch 10/15\n","104/104 [==============================] - 190s 2s/step - loss: 1.7034 - accuracy: 0.7837 - val_loss: 2.3572 - val_accuracy: 0.6683\n","Epoch 11/15\n","104/104 [==============================] - 191s 2s/step - loss: 1.7225 - accuracy: 0.7644 - val_loss: 2.3376 - val_accuracy: 0.6683\n","Epoch 12/15\n","104/104 [==============================] - 180s 2s/step - loss: 1.7231 - accuracy: 0.7788 - val_loss: 2.3075 - val_accuracy: 0.6683\n","Epoch 13/15\n","104/104 [==============================] - 180s 2s/step - loss: 1.7345 - accuracy: 0.7548 - val_loss: 2.2981 - val_accuracy: 0.6683\n","Epoch 14/15\n","104/104 [==============================] - 181s 2s/step - loss: 1.7228 - accuracy: 0.7644 - val_loss: 2.3194 - val_accuracy: 0.6683\n","Epoch 15/15\n","104/104 [==============================] - 180s 2s/step - loss: 1.7193 - accuracy: 0.8125 - val_loss: 2.3417 - val_accuracy: 0.6683\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","\n","/n 8056.41 /littleNtrain: 208 /n\n","Model: \"linear_model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_13 (InputLayer)       [(None, 250, 250, 3)]     0         \n","                                                                 \n"," model_4 (Functional)        (None, 256)               568368    \n","                                                                 \n"," dense_20 (Dense)            (None, 2)                 514       \n","                                                                 \n","=================================================================\n","Total params: 568,882\n","Trainable params: 565,874\n","Non-trainable params: 3,008\n","_________________________________________________________________\n","Epoch 1/15\n","108/108 [==============================] - 200s 2s/step - loss: 2.4013 - accuracy: 0.5694 - val_loss: 2.1040 - val_accuracy: 0.7163\n","Epoch 2/15\n","108/108 [==============================] - 197s 2s/step - loss: 2.1392 - accuracy: 0.6019 - val_loss: 2.0125 - val_accuracy: 0.6346\n","Epoch 3/15\n","108/108 [==============================] - 194s 2s/step - loss: 1.9619 - accuracy: 0.6991 - val_loss: 2.2486 - val_accuracy: 0.5721\n","Epoch 4/15\n","108/108 [==============================] - 186s 2s/step - loss: 1.8330 - accuracy: 0.7083 - val_loss: 3.7773 - val_accuracy: 0.4231\n","Epoch 5/15\n","108/108 [==============================] - 185s 2s/step - loss: 1.8616 - accuracy: 0.6574 - val_loss: 1.9228 - val_accuracy: 0.6250\n","Epoch 6/15\n","108/108 [==============================] - 185s 2s/step - loss: 1.8176 - accuracy: 0.7130 - val_loss: 1.9570 - val_accuracy: 0.6731\n","Epoch 7/15\n","108/108 [==============================] - 197s 2s/step - loss: 1.7651 - accuracy: 0.7361 - val_loss: 2.1199 - val_accuracy: 0.6442\n","Epoch 8/15\n","108/108 [==============================] - 186s 2s/step - loss: 1.7522 - accuracy: 0.7593 - val_loss: 2.1424 - val_accuracy: 0.6394\n","Epoch 9/15\n","108/108 [==============================] - 184s 2s/step - loss: 1.7592 - accuracy: 0.7361 - val_loss: 2.1680 - val_accuracy: 0.6490\n","Epoch 10/15\n","108/108 [==============================] - 184s 2s/step - loss: 1.7376 - accuracy: 0.7546 - val_loss: 2.2057 - val_accuracy: 0.6442\n","Epoch 11/15\n","108/108 [==============================] - 185s 2s/step - loss: 1.7555 - accuracy: 0.7407 - val_loss: 2.2029 - val_accuracy: 0.6442\n","Epoch 12/15\n","108/108 [==============================] - 184s 2s/step - loss: 1.7449 - accuracy: 0.7500 - val_loss: 2.2083 - val_accuracy: 0.6490\n","Epoch 13/15\n","108/108 [==============================] - 185s 2s/step - loss: 1.7410 - accuracy: 0.7593 - val_loss: 2.2123 - val_accuracy: 0.6442\n","Epoch 14/15\n","108/108 [==============================] - 185s 2s/step - loss: 1.7655 - accuracy: 0.7454 - val_loss: 2.2157 - val_accuracy: 0.6490\n","Epoch 15/15\n","108/108 [==============================] - 196s 2s/step - loss: 1.7740 - accuracy: 0.7407 - val_loss: 2.1928 - val_accuracy: 0.6442\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","SELF LESS TRAINING RESULTS SAVED!!!\n","\n","/n 10901.4 /littleNtrain: 216 /n\n"]}],"source":["t0 = time.time()\n","for littleNtrain in np.arange(192,217,8):\n","  x_train, y_train, x_test, y_test  = X[:littleNtrain], Y[:littleNtrain], X[-Ntest:], Y[-Ntest:]\n","\n","  class SimSiam(tf.keras.Model):\n","      def __init__(self, encoder, predictor):\n","          super().__init__()\n","          self.encoder = encoder\n","          self.predictor = predictor\n","          self.loss_tracker = tf.keras.metrics.Mean(name=\"loss\")\n","\n","      @property\n","      def metrics(self):\n","          return [self.loss_tracker]\n","\n","      def train_step(self, data):\n","          # Unpack the data.\n","          ds_one, ds_two = data\n","\n","          # Forward pass through the encoder and predictor.\n","          with tf.GradientTape() as tape:\n","              z1, z2 = self.encoder(ds_one), self.encoder(ds_two)\n","              p1, p2 = self.predictor(z1), self.predictor(z2)\n","              # Note that here we are enforcing the network to match\n","              # the representations of two differently augmented batches\n","              # of data.\n","              loss = compute_loss(p1, z2) / 2 + compute_loss(p2, z1) / 2\n","\n","          # Compute gradients and update the parameters.\n","          learnable_params = (\n","              self.encoder.trainable_variables + self.predictor.trainable_variables\n","          )\n","          gradients = tape.gradient(loss, learnable_params)\n","          self.optimizer.apply_gradients(zip(gradients, learnable_params))\n","\n","          # Monitor loss.\n","          self.loss_tracker.update_state(loss)\n","          return {\"loss\": self.loss_tracker.result()}\n","\n","  N = 2\n","  DEPTH = N * 9 + 2\n","  NUM_BLOCKS = ((DEPTH - 2) // 9) - 1\n","\n","\n","  def get_encoder():\n","      # Input and backbone.\n","      inputs = layers.Input((CROP_TO, CROP_TO, 3))\n","      x = layers.Rescaling(scale=1.0 / 127.5, offset=-1)(\n","          inputs\n","      )\n","      x = resnet_cifar10_v2.stem(x)\n","      x = resnet_cifar10_v2.learner(x, NUM_BLOCKS)\n","      x = layers.GlobalAveragePooling2D(name=\"backbone_pool\")(x)\n","\n","      # Projection head.\n","      x = layers.Dense(\n","          PROJECT_DIM, use_bias=False, kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n","      )(x)\n","      x = layers.BatchNormalization()(x)\n","      x = layers.ReLU()(x)\n","      x = layers.Dense(\n","          PROJECT_DIM, use_bias=False, kernel_regularizer=regularizers.l2(WEIGHT_DECAY)\n","      )(x)\n","      outputs = layers.BatchNormalization()(x)\n","      return tf.keras.Model(inputs, outputs, name=\"encoder\")\n","\n","\n","  def get_predictor():\n","      model = tf.keras.Sequential(\n","          [\n","              # Note the AutoEncoder-like structure.\n","              layers.Input((PROJECT_DIM,)),\n","              layers.Dense(\n","                  LATENT_DIM,\n","                  use_bias=False,\n","                  kernel_regularizer=regularizers.l2(WEIGHT_DECAY),\n","              ),\n","              layers.ReLU(),\n","              layers.BatchNormalization(),\n","              layers.Dense(PROJECT_DIM),\n","          ],\n","          name=\"predictor\",\n","      )\n","      return model\n","\n","      ###  TRAINING WITHOUT SELF SUPERVISED LEARNING\n","\n","  simsiam = SimSiam(get_encoder(), get_predictor())\n","\n","  # Create a cosine decay learning scheduler.\n","  num_training_samples = len(x_train)\n","  steps = EPOCHS * (num_training_samples // BATCH_SIZE)\n","  lr_decayed_fn = tf.keras.optimizers.schedules.CosineDecay(\n","      initial_learning_rate=0.03, decay_steps=steps\n","  )\n","\n","  # Create an early stopping callback.\n","  early_stopping = tf.keras.callbacks.EarlyStopping(\n","      monitor=\"loss\", patience=15, restore_best_weights=True\n","  )\n","\n","  def self_less_model(indim):\n","    inputs = layers.Input((indim, indim, 3))\n","    resnet = tf.keras.Model(\n","      simsiam.encoder.input, simsiam.encoder.get_layer(\"backbone_pool\").output\n","    )\n","    # We then create our linear classifier and train it.\n","    x = resnet(inputs)\n","    outputs = layers.Dense(num_cat, activation=\"softmax\")(x)\n","    linear_model = tf.keras.Model(inputs, outputs, name=\"linear_model\")\n","    # Compile model and start training.\n","    return linear_model\n","\n","  self_less = self_less_model(x_train[0].shape[0])\n","  self_less.trainable=True\n","  self_less.summary()\n","  self_less.compile(\n","      loss=\"categorical_crossentropy\",\n","      metrics=[\"accuracy\"],\n","      optimizer=tf.keras.optimizers.SGD(lr_decayed_fn, momentum=0.9),\n","  )\n","\n","  history = self_less.fit(x_train,y_train, epochs=EPOCHS, validation_data=(x_test, y_test), batch_size=2)\n","  pd.DataFrame(history.history).to_csv(loc + \"Results/selfLess_\"  + str(littleNtrain) + \"_\" + dataset_name + \".csv\")\n","  print('SELF LESS TRAINING RESULTS SAVED!!!\\n'*4)\n","  print(\"/n\",np.round(time.time() - t0,2),\"/littleNtrain:\", littleNtrain, \"/n\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jlVM0eHTdim6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1692763718898,"user_tz":240,"elapsed":28,"user":{"displayName":"David J Florez R","userId":"13874950378236808965"}},"outputId":"d1b7f5a1-87fb-4492-af2f-e88cf93e352d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["3141.592653589793"]},"metadata":{},"execution_count":8}],"source":["np.pi*1000"]},{"cell_type":"code","source":[],"metadata":{"id":"wqS-0_00usK2","executionInfo":{"status":"ok","timestamp":1692763718899,"user_tz":240,"elapsed":10,"user":{"displayName":"David J Florez R","userId":"13874950378236808965"}}},"execution_count":8,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1CjMzA-ssbYmytU-Ok_FVf2kgKdDUpjH4","timestamp":1692627289050},{"file_id":"1VjQdcYnRwpayd9ZEXsO5zvDIi7-cfjNp","timestamp":1692295262730},{"file_id":"1pMlFSDWe6d4x1yXCpq_Y8msZCQOUEmAg","timestamp":1691428617606}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}